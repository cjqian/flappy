{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkDA3uiF2lOBChsWRPxs4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjqian/flappy/blob/master/card_game/q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz71bjAs6Mrm"
      },
      "source": [
        "# The card game\n",
        "\n",
        "Let's wager on a card game. In front of you is a shuffled deck of 52 cards face-down. Half are red, half are black. Once you saw \"halt\", I'll flip over the next card and end the game. If the final card is red, you win \\$1. Otherwise, you lose \\$1. Let's play!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rqretej6AQR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux1Bss7q64I-"
      },
      "source": [
        "# Set up the game/environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogu-bEjg7Iv5"
      },
      "source": [
        "# The actions are to stop or to continue.\n",
        "ACTION_GO = 0\n",
        "ACTION_STOP = 1\n",
        "\n",
        "# The observation is the entire history of the game thus far. [RED, BLACK...]\n",
        "CARD_RED = 0\n",
        "CARD_BLACK = 1\n",
        "\n",
        "DECK_SIZE = 52\n",
        "\n",
        "class Environment():\n",
        "  def __init__(self):\n",
        "    self._dealt_cards = []\n",
        "    \n",
        "    # Same number of red and black cards.\n",
        "    red_cards = [CARD_RED] * int(DECK_SIZE / 2)\n",
        "    black_cards = [CARD_BLACK] * int(DECK_SIZE / 2)\n",
        "    self._deck = red_cards + black_cards\n",
        "    random.shuffle(self._deck)\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Returns reward, state, and whether the game is done.\"\"\"\n",
        "    done = False\n",
        "    if len(self._deck) == 1 or action == ACTION_STOP:\n",
        "      done = True\n",
        "      \n",
        "    card = self._deck.pop()\n",
        "    self._dealt_cards.append(card)\n",
        "\n",
        "    reward = 0\n",
        "\n",
        "  # The rewards are $1 if we win, -$1 if we lost,\n",
        "  # or $0 if the game has not ended.\n",
        "    if done:\n",
        "      reward = 1 if card == CARD_RED else -1\n",
        "    return (self._dealt_cards, reward, done)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJFe0NSSOypZ"
      },
      "source": [
        "# Approach 0: Random Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ZHsRLxXlOz92",
        "outputId": "56f90dab-a710-4e34-f991-2f68e296c1ef"
      },
      "source": [
        "RANDOM_REWARDS = []\n",
        "\n",
        "for episode in range(1, total_episodes+ 1):\n",
        "    if (episode % 10000) == 0:\n",
        "        print(\"Progress: {} ({})\".format(round((episode + 0.0)/total_episodes, 4),\n",
        "                                         round(sum(rewards) / len(rewards), 4)))\n",
        "    # Reset the environment\n",
        "    env = Environment()\n",
        "    state, reward, done = env.step(ACTION_GO)\n",
        "    state_h = get_hash_key(state)\n",
        "    step = 0\n",
        "    total_rewards = 0\n",
        "    \n",
        "    while not done:\n",
        "        action = random.randint(0, 1)\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done = env.step(action)\n",
        "        new_state_h = get_hash_key(new_state)\n",
        "\n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    RANDOM_REWARDS.append(total_rewards)\n",
        "    \n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "\n",
        "# Plot learning.\n",
        "pd.Series(RANDOM_REWARDS).expanding().mean().plot()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 0.2 (0.0022)\n",
            "Progress: 0.4 (0.0022)\n",
            "Progress: 0.6 (0.0022)\n",
            "Progress: 0.8 (0.0022)\n",
            "Progress: 1.0 (0.0022)\n",
            "Score over time: 0.0044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1ddb4bf668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYUklEQVR4nO3de5Bc5X3m8e+vr3OVZnQXICFuNpZtgtEEhxDbGESMhWuFtxybbHYtJ8sqWezdbBInJUe1STapTbGmkrVd9jqlYCcijtfGBC8kkMJCxrGTYMxgrgKDLggQSJqRhC5z6enbb//oo3HPqEfDzJlRj877fKqm5j3vefu87zvqfvroPT1zzN0REZHkSzV7ACIicmYo8EVEAqHAFxEJhAJfRCQQCnwRkUBkmj2AiSxatMhXrVrV7GGIiJxVHn/88UPuvrjRvjkb+KtWraK3t7fZwxAROauY2csT7dOSjohIIBT4IiKBUOCLiARCgS8iEggFvohIIBT4IiKBUOCLiAQikYFfrTp3PfYq5Uq12UMREZkzEhn433r8VX7v757mK//8UrOHIiIyZyQy8I8OlQA4PFhs8khEROaORAb+SS8cONHsIYiIzBmJDPydfQMA/NOL/U0eiYjI3JHIwM9nEjktEZFYEpmMLx7UUo6IyHiJDPy+EyPNHoKIyJyTyMB/+fBQs4cgIjLnJDLwRUTkVAp8EZFAKPBFRAKhwBcRCUTiA79a9WYPQURkTsg0ewCzrVStkk+lmz2MCb16ZIj/+o0neOKVo6N1v3ndJfzW9W+Z8DHuzsMv9PH+ty7BzMbU12+LiNSbkcA3sxuAzwNp4A53v23c/jxwJ7AGOAx8zN33zkTfkykUq+QzzQ38StV54cAJ/nzbizz0/MFJ239++04+v31n7H6//p/ezR/eu2P0T02Mt/G9F3Lfk69z4HiBW6+5iJvedS5vWdoJQKFUYaRcZX5rlmK5StWdfW8Mk04Z/7LrEFdfvIjOlgxPvnKUCxe3s2JBGykz0im94YjMVeYeb8nDzNLAi8D1wD7gMeCX3f25uja3Ape5+2+Y2c3Ah939Y6c7bk9Pj/f29k5rTKs23T9a/suP93D96qXTOs50+739I5fxyJ7D3PPj185Iv2ezRR05Dg3U/qrpuncu4/WjBZ589egp7ZbNayGXSfH7697GtZcuYXCkTKlaBYfjhTJf/t5uqu5cvKSDcsX52VXd/POuQ+zpH+ScrlbWnN/Nsvl5lnS20N2e4/DACEs6WxguVejIZzg6XOT4cInBkQoVd966tJNcJsVrbwyzZF6eE4UyHfkMbbk0u/sH2XtokGPDJVpzaV46NMjq5fPobs/Rkc9QKFUoV53jwyX6T4zwxlCRfW8M03eiwLJ5rXS2ZDgyWMSpvfbmt2ZZ1JGnPZfh1TeGKJarnNfdyrHhEulUiqo7Fy3uoKstSzpl5NIpytXaiUypUqXqcOBYgWKlQms2jTss7szTmktzwaJ2sukUKTPcnUy6torr7hwdKjEwUmawWOboUIn9x4Y5HP1b5DIpCqUKKTOy6Z+WB4tlSpUqmVSKkXKVwejxbblav63ZNG25NGbGcKnCiULtL9cuaM+xbF4LqZRRrTpVh5RBZ0sWADNoyaZZ0J6jVK7dx+LgiQLFcpVzulrpbstRrFQplau0ZNOYQVuu9rNuyaZY2J4nnTaODZU4NlxipFwBjAXtOcqVKoVSlVwmRTpllKtVqlU4OlxkoFCmLZchn03Rmk3TmktTrTpdbTmq7vSfGGFwpEyxUuX4cG2uQyNlCuUq+94YYqRUxYFMyihXnUrVKVaqFMtV8pkUnS1Z9h8bplCqkE2n6G7L1X6e5QqDI2WGixWGSxUAyhWnI5+hPZ/m4iUdbL5x9bReU2b2uLv3NNo3E2f4VwK73H1P1Nk3gPXAc3Vt1gN/FJXvBr5oZuZx320aGH/Tkzsf2TvtwB8qlvnVv3qMR186Mqb+xncu5/5n9gMwryXD8UJ5zP7fvfvp0x530wcvZcNVq9jdP8CK7jbmt2XH7K9Une3PH+SOH7zEZ9ZdytGhEu35DHv6B1i5sI0153eTz6RxdwZGyqMvmvrHX/T7D4xuf/L9F/Glh3dz9cUL+eT7L+aOH7zEd3/SN+WfB9SCoFieuRvLnAx7gAeeOTBhuwPHCwD8xtcen1Y/X/2X6d8bIZ0yKrNwLSidMvKZFEPFyowfeyLZtLGwPc/ASJmBkfLkD2jADNxr42/LpmnPZyiUa3MYLtb+Z3iyr9ZsLfyPF0rM/Ku9uea3ZmnN1lYPytUq2XTtDSWXSZFLp0Z/xufMb6U9n2a4VGbv4UEqFScXvRm05tJ0teUwam8aJwpl+gdG6G7PzcqYZ+IM/yPADe5+S7T9H4B3u/un6to8G7XZF23vjtocGnesjcBGgJUrV655+eWXpzye3f0DXPdn/zSmbu9tN7Jq0/1csbKLe269+k0d50sP7+L2B1+Ycv/jffd33kehVGX1OfNiHytJ3J3+gREWtec5NDjC7r5BLlrSzpLOlgnbv3pkmL999GV29Q3w2tFhjg6VOHC8wMevOp+VC9p418ouuttyPPPaMfpPjNDVlmP5/BY68hleOHiCh3/Sx49eOoIDv7h6KWaGGQyNlEmnUpzb3crSeXmGixWe33+CoWKZNed3c2SwyMuHh5jXmqVUqbKiu40VC1pxh4o7aTMWd+Y5PDjC4EiFfW8Mc9HidnKZFOcvbGdFdysL2nOYGYVShWKlSlsUhCmD148VGIhOGs5f2MbxQon9Rwss7szTkk0zOFLmwPECgyNlCqUq4OSzaQrF2lljueqc191Kay7NQKFMoVRhqFj72t0/MHoSVPXaUt2RoSKt2drZ/7zWLC3ZNJ35DOd1t7KoI0/KjJFyhVwmhTuUq7WAwqE9nx59A7QJlvBKleopy3vlSpX+gREMI5UCHIqVKoVSBYv+9zE4UhtbLvofycKO2tlw3/ECRwaLtTeYXIbhUoWq++hZc6lS5dBAkWrV6WjJ0N2WHV3GPTxYJJ2C1myGkXKFSjSXlBnzW7O05dKUKs5wqcJwscxQsUI6ZRwZLJJJGYs7a8+fbNqY15qlI5+hNZcmm0qdcqI2V8z2Gf6McfctwBaoLenM1HG/1fsqAD9+ZexSwUi5wr/uOsw1b1085mLnwEj5lLD/+i3v5o/+fgcvHmy8Hn7PrT/PBQvbZ+2dOWnMbDTcl3S2TBj09e1XLmzjM+veNumxL1zccUrdz6zo4qM9K6Y32BnUkk3Tkh17TencrtZT2tT/PBa051ixoO2MjO+nTh9mmfTE12qy6VM//JdJp1g+v7VB68ldsKh9Wo+TU81E4L8G1L+SzovqGrXZZ2YZYD61i7czrtHTsH6J5eDxAks68wwWK7zjDx8crf/zj/4MX/7e7lMucD73xx+gLVf7MX3nt943G0MWETkjZiLwHwMuMbMLqAX7zcC/G9fmPmAD8AjwEeC7s7F+/2a8+0+3A7ULRvV++66nTmn7vU9fMxr2IiJnu9hp5u5lM/sU8CC1j2V+1d13mNkfA73ufh/wFeBvzGwXcITam8KseLOfQ5/sGlz9mb2ISBLMSKK5+wPAA+Pq/qCuXAB+aSb6mk3f/933c7xQ4h3nzm/2UEREZlzi/rRCo/P7//6hxp9nvfbSJez+03X8zvVvYdf//CArF7Yp7EUksRIX+PXWnN8NwK9dvYqf/MkNAKy//JzR/Z/9yGWkU8Z/ue6S0V9IERFJqsQtUtcv4X/531/B7r5BzIyWbJq9t90IwP/+6OWcKJTn7OdoRURmQ+ICv95En+9OpUxhLyLBSdw6hjVcxRcRkeQFvvJeRKShxAW+iIg0psAXEQmEAl9EJBCJC3yt4YuINJa4wBcRkcYSF/i6ibeISGOJC3wREWkscYGv83sRkcYSF/giItJY4gJfS/giIo0lL/C1qCMi0lDiAl9ERBpLXOBrSUdEpLHEBb6IiDSWuMDXCb6ISGOJC3wREWkseYGvU3wRkYaSF/giItJQ4gJfn8MXEWkscYEvIiKNJS7w9Tl8EZHGYgW+mS0ws21mtjP63t2gzeVm9oiZ7TCzp83sY3H6nHRMs3lwEZGzWNwz/E3Adne/BNgebY83BHzc3d8O3AB8zsy6YvYrIiJTFDfw1wNbo/JW4KbxDdz9RXffGZVfB/qAxTH7nZDueCUi0ljcwF/q7vuj8gFg6ekam9mVQA7YPcH+jWbWa2a9/f39MYcmIiL1MpM1MLOHgGUNdm2u33B3NzM/zXGWA38DbHD3aqM27r4F2ALQ09Mz4bFERGTqJg18d1870T4zO2hmy919fxTofRO0mwfcD2x29x9Oe7QiIjJtcZd07gM2ROUNwL3jG5hZDvg2cKe73x2zPxERmaa4gX8bcL2Z7QTWRtuYWY+Z3RG1+SjwXuATZvZk9HV5zH5FRGSKJl3SOR13Pwxc16C+F7glKn8N+FqcfkREJL7E/aatiIg0psAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBCxAt/MFpjZNjPbGX3vPk3beWa2z8y+GKdPERGZnrhn+JuA7e5+CbA92p7InwDfj9mfiIhMU9zAXw9sjcpbgZsaNTKzNcBS4Dsx+3vTfv29F56prkREzgpxA3+pu++PygeohfoYZpYC/gz49GQHM7ONZtZrZr39/f2xBnZOV2usx4uIJE1msgZm9hCwrMGuzfUb7u5m5g3a3Qo84O77zOy0fbn7FmALQE9PT6NjiYjINE0a+O6+dqJ9ZnbQzJa7+34zWw70NWh2FfAeM7sV6AByZjbg7qdb7xcRkRk2aeBP4j5gA3Bb9P3e8Q3c/VdOls3sE0CPwl5E5MyLu4Z/G3C9me0E1kbbmFmPmd0Rd3AiIjJzYp3hu/th4LoG9b3ALQ3q/xr46zh9iojI9Og3bUVEAqHAFxEJhAJfRCQQCnwRkUAo8EVEAqHAFxEJhAJfRCQQCnwRkUAo8EVEAqHAFxEJhAJfRCQQCnwRkUAkLvDddd8UEZFGEhf4J01ycy0RkeAkNvBFRGQsBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhKIWIFvZgvMbJuZ7Yy+d0/QbqWZfcfMnjez58xsVZx+RURk6uKe4W8Ctrv7JcD2aLuRO4Hb3f1twJVAX8x+RURkiuIG/npga1TeCtw0voGZrQYy7r4NwN0H3H0oZr8iIjJFcQN/qbvvj8oHgKUN2rwFOGpm95jZE2Z2u5mlY/YrIiJTlJmsgZk9BCxrsGtz/Ya7u5k1ur9gBngP8C7gFeCbwCeArzToayOwEWDlypWTDU1ERKZg0sB397UT7TOzg2a23N33m9lyGq/N7wOedPc90WP+H/BzNAh8d98CbAHo6enRzWlFRGZQ3CWd+4ANUXkDcG+DNo8BXWa2ONq+FnguZr8iIjJFcQP/NuB6M9sJrI22MbMeM7sDwN0rwKeB7Wb2DGDAX8bsV0REpmjSJZ3TcffDwHUN6nuBW+q2twGXxelLRETi0W/aiogEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigVDgi4gEQoEvIhIIBb6ISCAU+CIigYgV+Ga2wMy2mdnO6Hv3BO0+a2Y7zOx5M/uCmVmcfkVEZOrinuFvAra7+yXA9mh7DDP7eeBq4DLgHcDPAu+L2a+IiExR3MBfD2yNyluBmxq0caAFyAF5IAscjNmviIhMUdzAX+ru+6PyAWDp+Abu/gjwMLA/+nrQ3Z9vdDAz22hmvWbW29/fH3NoIiJSLzNZAzN7CFjWYNfm+g13dzPzBo+/GHgbcF5Utc3M3uPuPxjf1t23AFsAenp6TjmWiIhM36SB7+5rJ9pnZgfNbLm77zez5UBfg2YfBn7o7gPRY/4RuAo4JfBFRGT2xF3SuQ/YEJU3APc2aPMK8D4zy5hZltoF24ZLOiIiMnviBv5twPVmthNYG21jZj1mdkfU5m5gN/AM8BTwlLv/fcx+RURkiiZd0jkddz8MXNegvhe4JSpXgF+P04+IiMSn37QVEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQCjwRUQCocAXEQmEAl9EJBAKfBGRQMQKfDP7JTPbYWZVM+s5TbsbzOwFM9tlZpvi9CkiItMT9wz/WeDfAt+fqIGZpYEvAR8EVgO/bGarY/YrIiJTlInzYHd/HsDMTtfsSmCXu++J2n4DWA88F6dvERGZmjOxhn8u8Grd9r6o7hRmttHMes2st7+/f1qdZTMp1r1zGSsXtE3r8SIiSTXpGb6ZPQQsa7Brs7vfO5ODcfctwBaAnp4en84x5rVk+T+/smYmhyUikgiTBr67r43Zx2vAirrt86I6ERE5g87Eks5jwCVmdoGZ5YCbgfvOQL8iIlIn7scyP2xm+4CrgPvN7MGo/hwzewDA3cvAp4AHgeeBu9x9R7xhi4jIVMX9lM63gW83qH8dWFe3/QDwQJy+REQkHv2mrYhIIBT4IiKBUOCLiARCgS8iEghzn9bvN806M+sHXo5xiEXAoRkaztkitDmHNl/QnEMRZ87nu/viRjvmbODHZWa97j7hX/BMotDmHNp8QXMOxWzNWUs6IiKBUOCLiAQiyYG/pdkDaILQ5hzafEFzDsWszDmxa/giIjJWks/wRUSkjgJfRCQQiQv8s/2G6Wb2VTPrM7Nn6+oWmNk2M9sZfe+O6s3MvhDN9Wkzu6LuMRui9jvNbENd/RozeyZ6zBdskvtTzjYzW2FmD5vZc2a2w8x+M6pP8pxbzOxHZvZUNOf/EdVfYGaPRuP8ZvTnxDGzfLS9K9q/qu5Yn4nqXzCzD9TVz8nXgZmlzewJM/uHaDvRczazvdFz70kz643qmvfcdvfEfAFpYDdwIZADngJWN3tcU5zDe4ErgGfr6j4LbIrKm4D/FZXXAf8IGPBzwKNR/QJgT/S9Oyp3R/t+FLW16LEfbPJ8lwNXROVO4EVqN7tP8pwN6IjKWeDRaHx3ATdH9X8B/OeofCvwF1H5ZuCbUXl19BzPAxdEz/30XH4dAL8NfB34h2g70XMG9gKLxtU17bmdtDP80Rumu3sROHnD9LOGu38fODKuej2wNSpvBW6qq7/Ta34IdJnZcuADwDZ3P+LubwDbgBuiffPc/Ydee7bcWXespnD3/e7+46h8gto9E84l2XN2dx+INrPRlwPXAndH9ePnfPJncTdwXXQmtx74hruPuPtLwC5qr4E5+Tows/OAG4E7om0j4XOeQNOe20kL/Dd9w/SzzFJ33x+VDwBLo/JE8z1d/b4G9XNC9N/2d1E74030nKOljSeBPmov4N3AUa/dMAjGjnN0btH+Y8BCpv6zaLbPAb8HVKPthSR/zg58x8weN7ONUV3TntuxboAiZ567u5kl7rO0ZtYB/B3w39z9eP1SZBLn7O4V4HIz66J2E6FLmzykWWVmHwL63P1xM7um2eM5g37B3V8zsyXANjP7Sf3OM/3cTtoZflJvmH4w+u8b0fe+qH6i+Z6u/rwG9U1lZllqYf+37n5PVJ3oOZ/k7keBh6ndJrTLzE6ehNWPc3Ru0f75wGGm/rNopquBf2Nme6ktt1wLfJ5kzxl3fy363kftjf1KmvncbvZFjZn8ovY/lj3ULuacvHDz9maPaxrzWMXYi7a3M/Yiz2ej8o2MvcjzI//pRZ6XqF3g6Y7KC7zxRZ51TZ6rUVt7/Ny4+iTPeTHQFZVbgR8AHwK+xdgLmLdG5U8y9gLmXVH57Yy9gLmH2sXLOf06AK7hpxdtEztnoB3orCv/K3BDM5/bTf/Hn4Uf8jpqn/TYDWxu9nimMf7/C+wHStTW5P4jtbXL7cBO4KG6f2wDvhTN9Rmgp+44v0btgtYu4Ffr6nuAZ6PHfJHot62bON9foLbO+TTwZPS1LuFzvgx4Iprzs8AfRPUXRi/gXVEQ5qP6lmh7V7T/wrpjbY7m9QJ1n9CYy68DxgZ+Yuccze2p6GvHyTE187mtP60gIhKIpK3hi4jIBBT4IiKBUOCLiARCgS8iEggFvohIIBT4IiKBUOCLiATi/wMAXT6r3rjrNwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vojZhIs5AUVm"
      },
      "source": [
        "# Approach 1: Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyeJNfbmAXND"
      },
      "source": [
        "# Initialize hyperparameters.\n",
        "\n",
        "total_episodes = 100000       # Total episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.005            # Exponential decay rate for exploration prob"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMTNm9rtA7lP",
        "outputId": "21c4d295-b9fb-4e27-fe81-dda354f9fb99"
      },
      "source": [
        "%%time\n",
        "\n",
        "def get_hash_key(state):\n",
        "  return ''.join([str(x) for x in state])\n",
        "\n",
        "# List of rewards\n",
        "rewards = []\n",
        "q_map  = {}\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(1, total_episodes+ 1):\n",
        "    if (episode % 10000) == 0:\n",
        "        print(\"Progress: {} ({})\".format(round((episode + 0.0)/total_episodes, 4),\n",
        "                                         round(sum(rewards) / len(rewards), 4)))\n",
        "    # Reset the environment\n",
        "    env = Environment()\n",
        "    state, reward, done = env.step(ACTION_GO)\n",
        "    state_h = get_hash_key(state)\n",
        "    step = 0\n",
        "    total_rewards = 0\n",
        "    \n",
        "    while not done:\n",
        "        exp_tradeoff = random.uniform(0, 1)\n",
        "        explore = exp_tradeoff <= epsilon\n",
        "\n",
        "        if state_h not in q_map:\n",
        "          q_map[state_h] = [0.0, 0.0]\n",
        "\n",
        "        if explore:\n",
        "          # Choose a random action.\n",
        "          action = random.randint(0, 1)\n",
        "        else:\n",
        "          action = np.argmax(q_map[state_h])\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done = env.step(action)\n",
        "        new_state_h = get_hash_key(new_state)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        if new_state_h not in q_map:\n",
        "          q_map[new_state_h] = [0.0, 0.0]\n",
        "\n",
        "        q_map[state_h][action] = q_map[state_h][action] + learning_rate * (\n",
        "            reward + gamma * np.max(q_map[new_state_h]) - q_map[state_h][action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "    \n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "\n",
        "# Plot learning.\n",
        "Q_LEARNING_REWARDS = rewards\n",
        "pd.Series(Q_LEARNING_REWARDS).expanding().mean().plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 0.1 (0.0031)\n",
            "Progress: 0.2 (0.0017)\n",
            "Progress: 0.3 (0.003)\n",
            "Progress: 0.4 (-0.0003)\n",
            "Progress: 0.5 (-0.0029)\n",
            "Progress: 0.6 (-0.0034)\n",
            "Progress: 0.7 (-0.0032)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNn3imP5GD7i"
      },
      "source": [
        "# Test it out!\n",
        "\n",
        "Hypothesis: q-learning isn't great for this; there is too large of a state space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "wZrrrT4-GCcZ",
        "outputId": "4968f95f-b5cd-4478-d1f6-0165bcc36a05"
      },
      "source": [
        "N_TRIALS = 100\n",
        "trial_rewards = []\n",
        "\n",
        "for episode in range(N_TRIALS):\n",
        "    env = Environment()\n",
        "    state, _, done = env.step(ACTION_GO)\n",
        "\n",
        "    while (not done):\n",
        "        state_h = get_hash_key(state)\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        if state_h in q_map: \n",
        "          action = np.argmax(q_map[state_h])\n",
        "        else:\n",
        "          action = ACTION_GO \n",
        "\n",
        "        new_state, reward, done = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # We print the number of step it took.\n",
        "    #        print(\"\\tEpisode {}: {}\".format(episode, reward))\n",
        "            trial_rewards.append(reward)\n",
        "            break\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "print(\"Payoff: {}\".format(sum(trial_rewards)))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-ec927a7d0958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Take the action (index) that have the maximum expected future reward given that state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_h\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTION_GO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \"\"\"\n\u001b[0;32m-> 1186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}