{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CardGame.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3JSQUBddOthdLr4NYsq22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjqian/flappy/blob/master/card_game.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz71bjAs6Mrm"
      },
      "source": [
        "# The card game\n",
        "\n",
        "Let's wager on a card game. In front of you is a shuffled deck of 52 cards face-down. Half are red, half are black. Once you saw \"halt\", I'll flip over the next card and end the game. If the final card is red, you win \\$1. Otherwise, you lose \\$1. Let's play!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rqretej6AQR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux1Bss7q64I-"
      },
      "source": [
        "# Set up the game/environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogu-bEjg7Iv5"
      },
      "source": [
        "# The actions are to stop or to continue.\n",
        "ACTION_GO = 0\n",
        "ACTION_STOP = 1\n",
        "\n",
        "# The observation is the entire history of the game thus far. [RED, BLACK...]\n",
        "CARD_RED = 0\n",
        "CARD_BLACK = 1\n",
        "\n",
        "DECK_SIZE = 52\n",
        "\n",
        "class Environment():\n",
        "  def __init__(self):\n",
        "    self._dealt_cards = []\n",
        "    \n",
        "    # Same number of red and black cards.\n",
        "    red_cards = [CARD_RED] * int(DECK_SIZE / 2)\n",
        "    black_cards = [CARD_BLACK] * int(DECK_SIZE / 2)\n",
        "    self._deck = red_cards + black_cards\n",
        "    random.shuffle(self._deck)\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Returns reward, state, and whether the game is done.\"\"\"\n",
        "    done = False\n",
        "    if len(self._deck) == 1 or action == ACTION_STOP:\n",
        "      done = True\n",
        "      \n",
        "    card = self._deck.pop()\n",
        "    self._dealt_cards.append(card)\n",
        "\n",
        "    reward = 0\n",
        "\n",
        "  # The rewards are $1 if we win, -$1 if we lost,\n",
        "  # or $0 if the game has not ended.\n",
        "    if done:\n",
        "      reward = 1 if card == CARD_RED else -1\n",
        "    return (self._dealt_cards, reward, done)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vojZhIs5AUVm"
      },
      "source": [
        "# Approach 1: Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyeJNfbmAXND"
      },
      "source": [
        "# Initialize hyperparameters.\n",
        "\n",
        "total_episodes = 20000       # Total episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.005            # Exponential decay rate for exploration prob"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMTNm9rtA7lP",
        "outputId": "86db9e97-b1e2-4744-f579-4a066645507e"
      },
      "source": [
        "def get_hash_key(state):\n",
        "  return ''.join([str(x) for x in state])\n",
        "\n",
        "# List of rewards\n",
        "rewards = []\n",
        "q_map  = {}\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    env = Environment()\n",
        "    state, reward, done = env.step(ACTION_GO)\n",
        "    state_h = get_hash_key(state)\n",
        "    step = 0\n",
        "    total_rewards = 0\n",
        "    \n",
        "    while not done:\n",
        "        exp_tradeoff = random.uniform(0, 1)\n",
        "        explore = exp_tradeoff <= epsilon\n",
        "\n",
        "        if state_h not in q_map:\n",
        "          q_map[state_h] = [0.0, 0.0]\n",
        "\n",
        "        if explore:\n",
        "          # Choose a random action.\n",
        "          action = random.randint(0, 1)\n",
        "        else:\n",
        "          action = np.argmax(q_map[state_h])\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done = env.step(action)\n",
        "        new_state_h = get_hash_key(new_state)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        if new_state_h not in q_map:\n",
        "          q_map[new_state_h] = [0.0, 0.0]\n",
        "\n",
        "        q_map[state_h][action] = q_map[state_h][action] + learning_rate * (\n",
        "            reward + gamma * np.max(q_map[new_state_h]) - q_map[state_h][action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "    \n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: -0.0127\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}